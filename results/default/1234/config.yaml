data:
  batch_size: 1
  data_root_dir: /u/shirlywang/vqa/pythia/data
  dataset: vqa_2.0
  image_depth_first: false
  image_fast_reader: false
  image_feat_test:
  - features/test
  image_feat_train:
  - features/train
  image_feat_val:
  - features/val
  image_max_loc: 100
  imdb_file_test:
  - imdb/imdb_test2015.npy
  imdb_file_train:
  - imdb/imdb_train2014.npy
  imdb_file_val:
  - imdb/imdb_val2014.npy
  num_workers: 1
  pretrained_bert_dir: /u/lxiao/VQA/pythia/pretrained_bert
  question_max_len: 14
  vocab_answer_file: answers_vqa.txt
  vocab_question_file: vocabulary_vqa.txt
exp_name: baseline
loss: logitBCE
model:
  classifier:
    method: logit_classifier
    par:
      img_hidden_dim: 5000
      txt_hidden_dim: 300
  image_embedding_models:
  - modal_combine:
      method: non_linear_elmt_multiply
      par:
        dropout: 0
        hidden_size: 5000
    normalization: softmax
    transform:
      method: linear_transform
      par:
        out_dim: 1
  image_feat_dim: 2048
  image_feature_encoding:
  - method: default_image
    par: {}
  modal_combine:
    method: non_linear_elmt_multiply
    par:
      dropout: 0
      hidden_size: 5000
  question_embedding:
  - method: bert_embed
    par:
      attention_probs_dropout_prob: 0.1
      hidden_act: gelu
      hidden_dropout_prob: 0.1
      hidden_size: 768
      initializer_range: 0.02
      intermediate_size: 3072
      max_position_embeddings: 512
      num_attention_heads: 12
      num_hidden_layers: 12
      pretrained_bert_dir: /u/lxiao/VQA/pythia/pretrained_bert
      q_embedding_size: 2048
      type_vocab_size: 2
      vocab_size: 30522
optimizer:
  method: Adamax
  par:
    eps: 1.0e-08
    lr: 0.01
    weight_decay: 0
run: train+val
training_parameters:
  clip_norm_mode: all
  lr_ratio: 0.1
  lr_steps:
  - 5000
  - 7000
  - 9000
  - 11000
  max_grad_l2_norm: 0.25
  max_iter: 12000
  report_interval: 100
  snapshot_interval: 1000
  wu_factor: 0.2
  wu_iters: 1000
